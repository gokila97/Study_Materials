{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225f0d9a",
   "metadata": {},
   "source": [
    "# Popular Natural Language Processing Text Preprocessing Techniques Implementation In Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be6202",
   "metadata": {},
   "source": [
    "Using the text preprocessing techniques we can remove noise from raw data and makes raw data more valuable for building models. \n",
    "\n",
    "Here, raw data is nothing but data we collect from different sources like reviews from websites, documents, social media, twitter tweets, news articles etc. \n",
    "\n",
    "Data preprocessing is the primary and most crucial step in any data science problems or project. Preprocessing the collected data is the integral part of any Natural Language Processing, Computer Vision, deep learning and machine learning problems. Based on the type of dataset, we have to follow different preprocessing methods. \n",
    "\n",
    "Which means machine learning data preprocessing techniques vary from the deep learning, natural language or nlp  data preprocessing techniques.\n",
    "\n",
    "So there is a need to learn these techniques to build effective natural language processing models.\n",
    "\n",
    "In this article we will discuss different text preprocessing techniques or methods like normalization, stemming, lemmatization, etc. for  handling text to build various Natural Language Processing problems/models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791b108",
   "metadata": {},
   "source": [
    "# Text Preprocessing Importance in NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6f3a0",
   "metadata": {},
   "source": [
    "we said before text preprocessing is the first step in the Natural Language Processing pipeline. The importance of preprocessing is increasing in NLP due to noise or unclear data extracted or collected from different sources. \n",
    "\n",
    "Most of the text data collected from reviews of E-commerce websites like Amazon or Flipkart, tweets from twitter,  comments from Facebook or Instagram, and other websites like Wikipedia, etc. \n",
    "\n",
    "We can observe users use short forms, emojis, misspelling of words, etc. in their comments, tweets, and so on.\n",
    "\n",
    "We should not feed raw data without preprocessing to  build models because the preprocessing of text directly improves the model's performance.\n",
    "\n",
    "If we feed data without performing any text preprocessing techniques, the build models will not learn the real significance of the data. In some cases, if we feed raw data without any preprocessing techniques the models will get confused and give random results. \n",
    "\n",
    "In that confusion, the model will learn harmful patterns that are not valuable. Due to this, the model's performance will be affected, which means the model performance will reduce significantly.\n",
    "\n",
    "So we should remove all these noises from the text and make it a more clear and structured form for building models.\n",
    "\n",
    "Here we have to know one thing.\n",
    "\n",
    "The natural language text preprocessing techniques will vary from problem to problem. This means we cannot apply the same text preprocessing techniques used for one NLP problem to another NLP problem. \n",
    "\n",
    "For example, in sentiment analysis classification problems, we can remove or ignore numbers within the text because numbers are not significant in this problem statement.\n",
    "\n",
    "However, we should not ignore the numbers if we are dealing with financial related problems. Because numbers play a key role in these kinds of problems.\n",
    "\n",
    "So while performing NLP text preprocessing techniques. We need to focus more on the domain we are applying these NLP techniques and the order of methods also plays a key role.\n",
    "\n",
    "Don't worry about the order of these techniques for now.  We will give the generic order in which you need to apply these techniques.\n",
    "\n",
    "Our suggestion is to use preprocessing methods or techniques on a subset of aggregate data (take a few sentences randomly). We can easily observe whether it is in our expected form or not. If it is in our expected form, then apply on a complete dataset; otherwise, change the order of preprocessing techniques.\n",
    "\n",
    "We will provide a python file with a preprocess class of all preprocessing techniques at the end of this article.\n",
    "\n",
    "You can download and import that class to your code. We can get preprocessed text by calling preprocess class with a list of sentences and sequences of preprocessing techniques we need to use.\n",
    "\n",
    "Again the order of technique we need to use will differ from problem to problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf086945",
   "metadata": {},
   "source": [
    "# Different Text Preprocessing Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451f937",
   "metadata": {},
   "source": [
    "Let us jump to learn different types of text preprocessing techniques. \n",
    "\n",
    "In the next few minutes, we will discuss and learn the importance and implementation of these techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f614017",
   "metadata": {},
   "source": [
    "**Converting to Lower case**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b29b9",
   "metadata": {},
   "source": [
    "Converting all our text into the lower case is a simple and most effective approach.  If we are not applying lower case conversion on words like NLP, nlp, Nlp, we are treating all these words as different words. \n",
    "\n",
    "After using the lower casing, all three words are treated as a single word that is nlp.!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e5e01",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce250129",
   "metadata": {},
   "source": [
    "This method is useful for problems that are dependent on the frequency of words such as document classification. \n",
    "\n",
    "In this case, we count the frequency of words by using bag-of-words, TFIDF, etc.\n",
    "\n",
    "It is better to perform lower case the text as the first step in this text preprocessing. Because if we are trying to remove stop words all words need to be in lower case.\n",
    "\n",
    "For example, few sentences have the starting word as \"The\" if we are not performing the lower casing technique before that technique, we can not remove all stopwords. \n",
    "\n",
    "The other case is for calculating the frequency count. If we not converted the text into lower case Data Science and data science will treat as different tokens.\n",
    "\n",
    "In natural language processing the lower dimension of text which is words called as tokens.\n",
    "\n",
    "We can apply this method to most of the text related problems. Still, it may not be suitable for different projects like Parts-Of-Speech tag recognition or dependency parsing, where proper word casing is essential to recognize nouns, verbs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18768093",
   "metadata": {},
   "source": [
    "**Implementation of lower case conversion**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1525eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example sentence for lower case conversion\n"
     ]
    }
   ],
   "source": [
    "# Implementation of lower case conversion\n",
    "\n",
    "def lower_case_convertion(text):\n",
    "\t\"\"\"\n",
    "\tInput :- string\n",
    "\tOutput :- lowercase string\n",
    "\t\"\"\"\n",
    "\tlower_text = text.lower()\n",
    "\treturn lower_text\n",
    "\n",
    "\n",
    "ex_lowercase = \"This is an example Sentence for LOWER case conversion\"\n",
    "lowercase_result = lower_case_convertion(ex_lowercase)\n",
    "print(lowercase_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69715e7a",
   "metadata": {},
   "source": [
    "# Removal of HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3365db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebea4186",
   "metadata": {},
   "source": [
    "This is the second essential preprocessing technique. The chances to get HTML tags in our text data is quite common when we are extracting or scraping data from different websites. \n",
    "\n",
    "We don't get any valuable information from these HTML tags. So it is better to remove them from our text data. We can remove these tags by using regex and we can also use the BeautifulSoup module from bs4 libraries. \n",
    "\n",
    "Let us see the implementation using python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c0f39",
   "metadata": {},
   "source": [
    "**HTML tags removal Implementation using regex module**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b354b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result :- \n",
      "   \n",
      " \n",
      " Hi, this is an example text with Html tags.  \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HTML tags removal Implementation using regex module\n",
    "\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without Html tags\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\thtml_pattern = r'<.*?>'\n",
    "\twithout_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n",
    "\treturn without_html\n",
    "\n",
    "ex_htmltags = \"\"\" <body>\n",
    "<div>\n",
    "<h1>Hi, this is an example text with Html tags. </h1>\n",
    "</div>\n",
    "</body>\n",
    "\"\"\"\n",
    "htmltags_result = remove_html_tags(ex_htmltags)\n",
    "print(f\"Result :- \\n {htmltags_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b28b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b3a5683",
   "metadata": {},
   "source": [
    "**Implementation of Removing HTML tags using bs4 library**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0994a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result :- \n",
      "   \n",
      " \n",
      " Hi, this is an example text with Html tags.  \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Removing HTML tags using bs4 library\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags_beautifulsoup(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without Html tags\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\tparser = BeautifulSoup(text, \"html.parser\")\n",
    "\twithout_html = parser.get_text(separator = \" \")\n",
    "\treturn without_html\n",
    "\n",
    "ex_htmltags = \"\"\" <body>\n",
    "<div>\n",
    "<h1>Hi, this is an example text with Html tags. </h1>\n",
    "</div>\n",
    "</body>\n",
    "\"\"\"\n",
    "htmltags_result = remove_html_tags_beautifulsoup(ex_htmltags)\n",
    "print(f\"Result :- \\n {htmltags_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f010ecc",
   "metadata": {},
   "source": [
    "We can observe both the functions are giving the same result after removing HTML tags from our example text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb2bca",
   "metadata": {},
   "source": [
    "**Removal of URLs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1864a5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62949a15",
   "metadata": {},
   "source": [
    "URL is the short-form of Uniform Resource Locator. The URLs within the text refer to the location of another website or anything else.\n",
    "\n",
    "If we are performing any website backlinks analysis, twitter or Facebook in that case, URLs are an excellent choice to keep in text.\n",
    "\n",
    "Otherwise, from URLs also we can not get any information. So we can remove it from our text. We can remove URLs from the text by using the python Regex library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d62a41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c90de37",
   "metadata": {},
   "source": [
    "**Implementation of Removing URLs  using python regex**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a59b86e",
   "metadata": {},
   "source": [
    "In the below script. We take example text with URLs and then call the 2 functions with that example text. In the remove_urls function, assign a regular expression to remove URLs to url_pattern after That, substitute URLs within the text with space by calling the re library's sub-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9fdda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after removing URLs from text :- \n",
      " \n",
      "This is an example text for URLs like   &   etc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Removing URLs  using python regex\n",
    "\n",
    "import re\n",
    "def remove_urls(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without URLs\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\turl_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\twithout_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "\treturn without_urls\n",
    "\n",
    "# example text which contain URLs in it\n",
    "ex_urls = \"\"\"\n",
    "This is an example text for URLs like http://google.com & https://www.facebook.com/ etc.\n",
    "\"\"\"\n",
    "\n",
    "# calling removing_urls function with example text (ex_urls)\n",
    "urls_result = remove_urls(ex_urls)\n",
    "print(f\"Result after removing URLs from text :- \\n {urls_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59d73f",
   "metadata": {},
   "source": [
    "**Removing Numbers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbabb0",
   "metadata": {},
   "source": [
    "We can remove numbers from the text if our problem statement doesn't require numbers. \n",
    "\n",
    "For example, if we are working on financial related problems like banking or insurance-related sectors. We may get information from numbers.\n",
    "\n",
    "In those cases, we shouldn't remove numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ca704",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dd8ff96",
   "metadata": {},
   "source": [
    "**Implementation of Removing numbers  using python regex**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2047ba",
   "metadata": {},
   "source": [
    "In the code below, we will call the remove_numbers function with example text, which contains numbers.\n",
    "\n",
    "Let's see how to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202219d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after removing number from text :- \n",
      " \n",
      "This is an example sentence for removing numbers like  ,  , ,   ,  etc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Removing numbers  using python regex\n",
    "\n",
    "import re\n",
    "def remove_numbers(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without numbers\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\tnumber_pattern = r'\\d+'\n",
    "\twithout_number = re.sub(pattern=number_pattern,\n",
    " repl=\" \", string=text)\n",
    "\treturn without_number\n",
    "\n",
    "# example text which contain numbers in it\n",
    "ex_numbers = \"\"\"\n",
    "This is an example sentence for removing numbers like 1, 5,7, 4 ,77 etc.\n",
    "\"\"\"\n",
    "# calling remove_numbers function with example text (ex_numbers)\n",
    "numbers_result = remove_numbers(ex_numbers)\n",
    "print(f\"Result after removing number from text :- \\n {numbers_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cc76e",
   "metadata": {},
   "source": [
    "In the above removing_numbers function. We mentioned a pattern to recognize numbers within the text and then substitute numbers with space using the re library's sub-function.\n",
    "\n",
    "And then return text after removing the number to numbers_result variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9e820",
   "metadata": {},
   "source": [
    "**Converting numbers to words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1ba46",
   "metadata": {},
   "source": [
    "our problem statement need valuable information from numbers in that case, we have to convert numbers to words. Similar problem statements which are discussed at the removing numbers (above section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fba0b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b13867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4fd1ed3",
   "metadata": {},
   "source": [
    "**Implementation of Converting numbers to words using python num2words library**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d02cd2",
   "metadata": {},
   "source": [
    "We can convert numbers to words by just importing the num2words library. In the code below, we will call the num_to_words function with example text. Example text has numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34c2f9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num2words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m ex_numbers \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124mThis is an example sentence for converting numbers to words like 1 to one, 5 to five, 74 to seventy-four, etc.\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# calling remove_numbers function with example text (ex_numbers)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m numners_result \u001b[38;5;241m=\u001b[39m \u001b[43mnum_to_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex_numbers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult after converting numbers to its words from text :- \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumners_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mnum_to_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(after_spliting)):\n\u001b[0;32m     12\u001b[0m \t\t\u001b[38;5;28;01mif\u001b[39;00m after_spliting[index]\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m---> 13\u001b[0m \t\t\tafter_spliting[index] \u001b[38;5;241m=\u001b[39m \u001b[43mnum2words\u001b[49m(after_spliting[index])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# joining list into string with space\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \tnumbers_to_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(after_spliting)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num2words' is not defined"
     ]
    }
   ],
   "source": [
    "# function to convert numbers to words\n",
    "def num_to_words(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text which have all numbers or integers in the form of words\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\t# splitting text into words with space\n",
    "\tafter_spliting = text.split()\n",
    "\n",
    "\tfor index in range(len(after_spliting)):\n",
    "\t\tif after_spliting[index].isdigit():\n",
    "\t\t\tafter_spliting[index] = num2words(after_spliting[index])\n",
    "\n",
    "    # joining list into string with space\n",
    "\tnumbers_to_words = ' '.join(after_spliting)\n",
    "\treturn numbers_to_words\n",
    "\n",
    "# example text which contain numbers in it\n",
    "ex_numbers = \"\"\"\n",
    "This is an example sentence for converting numbers to words like 1 to one, 5 to five, 74 to seventy-four, etc.\n",
    "\"\"\"\n",
    "# calling remove_numbers function with example text (ex_numbers)\n",
    "numners_result = num_to_words(ex_numbers)\n",
    "print(f\"Result after converting numbers to its words from text :- \\n {numners_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33d47d",
   "metadata": {},
   "source": [
    "In the above code, the num_to_words function is getting the text as input. In that, we are splitting text using a python string function of a split with space to get words individually.  \n",
    "\n",
    "Taking each word and checking if that word is digit or not. If the word is digit then convert that into words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e07389",
   "metadata": {},
   "source": [
    "**Apply spelling correction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b19cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed98d3dd",
   "metadata": {},
   "source": [
    "Spelling correction is another important preprocessing technique while working with tweets, comments, etc. Because we can see incorrect spelling words in those areas of text. We need to make those misspelling words to correct spelling words.\n",
    "\n",
    "We can check and replace misspelling words with correct spelling by using two python libraries, one is pyspellchecker, and another one is autocorrect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acd343",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "445cbbbc",
   "metadata": {},
   "source": [
    "**Implementation of spelling correction using python pyspellchecker library**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7ac1c",
   "metadata": {},
   "source": [
    "Below we are calling a spell_correction function with example text. Example text has incorrect spelling words to check whether the spell_correction function gives correct words or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6cb7945",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spellchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of spelling correction using python pyspellchecker library\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspellchecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[0;32m      5\u001b[0m spell_corrector \u001b[38;5;241m=\u001b[39m SpellChecker()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# spelling correction using spellchecker\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spellchecker'"
     ]
    }
   ],
   "source": [
    "# Implementation of spelling correction using python pyspellchecker library\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell_corrector = SpellChecker()\n",
    "\n",
    "# spelling correction using spellchecker\n",
    "def spell_correction(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text which have correct spelling words\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\t# initialize empty list to save correct spell words\n",
    "\tcorrect_words = []\n",
    "\t# extract spelling incorrect words by using unknown function of spellchecker\n",
    "\tmisSpelled_words = spell_corrector.unknown(text.split())\n",
    "\n",
    "\tfor each_word in text.split():\n",
    "\t\tif each_word in misSpelled_words:\n",
    "\t\t\tright_word = spell_corrector.correction(each_word)\n",
    "\t\t\tcorrect_words.append(right_word)\n",
    "\t\telse:\n",
    "\t\t\tcorrect_words.append(each_word)\n",
    "\n",
    "\t# joining correct_words list into single string\n",
    "\tcorrect_spelling = ' '.join(correct_words)\n",
    "\treturn correct_spelling\n",
    "\n",
    "#example text with mis spelling words\n",
    "ex_misSpell_words = \"\"\"\n",
    "This is an example sentence for spell corecton\n",
    "\"\"\"\n",
    "spell_result = spell_correction(ex_misSpell_words)\n",
    "print(f\"Result after spell checking :- \\n{spell_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e4ee3",
   "metadata": {},
   "source": [
    "**Implementation of spelling correction using python autocorrect library**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137beb51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autocorrect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of spelling correction using python autocorrect library\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautocorrect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Speller\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# spelling correction using spellchecker\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autocorrect'"
     ]
    }
   ],
   "source": [
    "# Implementation of spelling correction using python autocorrect library\n",
    "\n",
    "from autocorrect import Speller\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# spelling correction using spellchecker\n",
    "def spell_autocorrect(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text which have correct spelling words\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\tcorrect_spell_words = []\n",
    "\n",
    "\t# initialize Speller object for english language with 'en'\n",
    "\tspell_corrector = Speller(lang='en')\n",
    "\tfor word in word_tokenize(text):\n",
    "\t\t# correct spell word\n",
    "\t\tcorrect_word = spell_corrector(word)\n",
    "\t\tcorrect_spell_words.append(correct_word)\n",
    "\n",
    "\tcorrect_spelling = ' '.join(correct_spell_words)\n",
    "\treturn correct_spelling\n",
    "\n",
    "# another example text with misSpelling words\n",
    "ex_misSpell_words_1 = \"\"\"\n",
    "This is anoter exapl for spell correction\n",
    "\"\"\"\n",
    "spell_result = spell_autocorrect(ex_misSpell_words_1)\n",
    "print(f\"Result :- \\n{spell_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28aae3",
   "metadata": {},
   "source": [
    "We can observe both methods given correct or expected solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034e93db",
   "metadata": {},
   "source": [
    "**We can observe both methods given correct or expected solutions**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ecf7f",
   "metadata": {},
   "source": [
    "This is another common preprocessing technique in NLP. We can observe special characters at the top of the common letter or characters if we press a longtime while typing, for example, résumé. \n",
    "\n",
    "If we are not removing these types of noise from the text, then the model will consider resume and résumé; both are two different words.\n",
    "\n",
    "Even if both are the same. We can convert this accented character to ASCII characters by using the unidecode library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dc91a",
   "metadata": {},
   "source": [
    "**Implementation of accented text to ASCII converter in python**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e28964",
   "metadata": {},
   "source": [
    "We will define the accented_to_ascii function to convert accented characters to their ASCII values in the below script.  \n",
    "\n",
    "We will do this function with example text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c923ff8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of accented text to ASCII converter in python\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01munidecode\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccented_to_ascii\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\tReturn :- text after converting accented characters\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\tInput :- string\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\tOutput :- string\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "# Implementation of accented text to ASCII converter in python\n",
    "\n",
    "import unidecode\n",
    "\n",
    "def accented_to_ascii(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text after converting accented characters\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\t# apply unidecode function on text to convert\n",
    "\t# accented characters to ASCII values\n",
    "\ttext = unidecode.unidecode(text)\n",
    "\treturn text\n",
    "\n",
    "# example text with accented characters\n",
    "ex_accented = \"\"\"\n",
    "This is an example text with accented characters like dèèp lèarning ánd cömputer vísíön etc.\n",
    "\"\"\"\n",
    "accented_result = accented_to_ascii(ex_accented)\n",
    "print(f\"Result after converting accented characters to their ASCII values \\n{accented_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f35f9",
   "metadata": {},
   "source": [
    "The above code, we use the unidecode method of the unidecode library with input text. Which converts accented characters to ASCII values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d9c45",
   "metadata": {},
   "source": [
    "**Converting chat conversion words to normal words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838a008",
   "metadata": {},
   "source": [
    "This is another essential preprocessing technique if we work with chat conversions, or our problem statement requires chat conversion analysis. We need to handle short-form. As nowadays, people use short-form words in their chatting conversions for their simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98bc50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2d1b75",
   "metadata": {},
   "source": [
    "better way to work with those words is to replace short-form words to their original words.\n",
    "\n",
    "We can find all those short-form words and its actual words in this Github Repo to save that file into our system; click right click and then press on save as option.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fb3bd",
   "metadata": {},
   "source": [
    "**Implementation of python script**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7929c63b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'short_forms.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m ex_chat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124momg this is an example text for chat conversation.\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# open short_form file and then read sentences from text file using read())\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m short_form_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshort_forms.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m chat_words_str \u001b[38;5;241m=\u001b[39m short_form_list\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     26\u001b[0m chat_words_map_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'short_forms.txt'"
     ]
    }
   ],
   "source": [
    "# Converting chat conversion words to normal words\n",
    "\n",
    "def short_to_original(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text after converting short_form words to original\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\tnew_text = []\n",
    "\tfor w in text.split():\n",
    "\t\tif w.upper() in chat_words_list:\n",
    "\t\t\tnew_text.append(chat_words_map_dict[w.upper()])\n",
    "\t\telse:\n",
    "\t\t\tnew_text.append(w)\n",
    "\treturn \" \".join(new_text)\n",
    "\n",
    "\n",
    "# example text for chat conversation short-form words\n",
    "ex_chat = \"\"\"\n",
    "omg this is an example text for chat conversation.\n",
    "\"\"\"\n",
    "# open short_form file and then read sentences from text file using read())\n",
    "short_form_list = open('short_forms.txt', 'r')\n",
    "chat_words_str = short_form_list.read()\n",
    "\n",
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "\tif line != \"\":\n",
    "\t\tcw = line.split(\"=\")[0]\n",
    "\t\tcw_expanded = line.split(\"=\")[1]\n",
    "\t\tchat_words_list.append(cw)\n",
    "\t\tchat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "\n",
    "# calling function\n",
    "chat_result = short_to_original(ex_chat)\n",
    "print(f\"Result {chat_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a2147",
   "metadata": {},
   "source": [
    "**Expanding Contractions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4722a",
   "metadata": {},
   "source": [
    "Contractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\n",
    "\n",
    "An example of a contraction word.\n",
    "\n",
    "\"don't\" is \"do not\" \n",
    "\"should've\" is \"should have\" \n",
    "Nlp models don't know about these contractions; they will consider \"don't\" and \"do not\" both are two different words.\n",
    "\n",
    "We have to choose this technique if our problem statement is required. Otherwise,  leave it as it is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d8521",
   "metadata": {},
   "source": [
    "**Implementation of expanding contractions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2810c",
   "metadata": {},
   "source": [
    "The code below, we are importing the CONTRACTION_MAP dictionary from the contraction file. And then define expand_contractions function to expand contractions if our input text has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63ad331d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Implementation of expanding contractions\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONTRACTION_MAP\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand_contractions\u001b[39m(contraction):\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;66;03m# take matching contraction in the text\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \tmatch \u001b[38;5;241m=\u001b[39m contraction\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contraction'"
     ]
    }
   ],
   "source": [
    "## Implementation of expanding contractions\n",
    "\n",
    "from contraction import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(contraction):\n",
    "\t# take matching contraction in the text\n",
    "\tmatch = contraction.group(0)\n",
    "\t# first char from matching contraction (D for Doesn't)\n",
    "\tfirst_char = match[0]\n",
    "\tif contraction_mapping.get(match):\n",
    "\t\texpanded_contraction = contraction_mapping.get(match)\n",
    "\telse:\n",
    "\t\texpanded_contraction = contraction_mapping.get(match.lower())\n",
    "\texpanded_contraction = first_char+expanded_contraction[1:]\n",
    "\n",
    "\treturn expanded_contraction\n",
    "\n",
    "# expending contractions\n",
    "contraction_mapping = CONTRACTION_MAP\n",
    "# take all key values from contraction_mapping\n",
    "contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "# example text with contractions\n",
    "ex_contractions = \"\"\"\n",
    "Sometimes our mind doesn't work properly.\n",
    "\"\"\"\n",
    "# substitute result of function in the text\n",
    "expanded_text = contractions_pattern.sub(expand_contractions, ex_contractions)\n",
    "# replacing apostrophe with empty string (to remove apostrophe)\n",
    "expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "print(f\"Result :- \\n{expanded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ade30",
   "metadata": {},
   "source": [
    "e can observe in the output, the contraction of \"doesn't\" in the example text expanded to \"does not\".\n",
    "\n",
    "In the expand_contractions function, we take contraction words from our text matching with contraction map words. If we are not performing a lower case conversion technique before this, we have to take the first character to display the result of contraction \"Doesn't\" like \"Does not\".\n",
    "\n",
    "Otherwise, we can ignore a few steps in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a0198",
   "metadata": {},
   "source": [
    "**Stemming**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c691d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab278ebd",
   "metadata": {},
   "source": [
    "Stemming is reducing words to their base or root form by removing a few suffix characters from words. Stemming is the text normalization technique.\n",
    "\n",
    "There are so many stemming algorithms available, but the most widely used one is porter stemming.\n",
    "\n",
    "For example, the result of books after stemming is a book, and the result of learning is learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81ea45",
   "metadata": {},
   "source": [
    "But stemming doesn't always provide the correct form of words because this follows the rules like removing suffix characters to get base words.\n",
    "\n",
    "Sometimes, stemming words don't relate to original ones and sometimes give non - dictionary words or not proper words.  \n",
    "\n",
    "For this, we can observe in the above table results of stemming \"caring\" and \"console/consoling\". Because of these results stemming technique does not apply to all NLP tasks.\n",
    "\n",
    "Implementation of Stemming using PorterStemming from nltk library\n",
    "In the below python script, we will define the porter_stemmer function to implement the stemming technique. We will call the function with example text.\n",
    "\n",
    "Before reaching the function, we have to initialize the object for the PorterStemmer class to use the stem function from that class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a965b",
   "metadata": {},
   "source": [
    "**Implementation of Stemming using PorterStemming from nltk library**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5f8b9",
   "metadata": {},
   "source": [
    "In the below python script, we will define the porter_stemmer function to implement the stemming technique. We will call the function with example text.\n",
    "\n",
    "Before reaching the function, we have to initialize the object for the PorterStemmer class to use the stem function from that class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00f3651c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of Stemming using PorterStemming from nltk library\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mporter_stemmer\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\tResult :- string after stemming\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\tInput :- String\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\tOutput :- String\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Implementation of Stemming using PorterStemming from nltk library\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(text):\n",
    "\t\"\"\"\n",
    "\tResult :- string after stemming\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\t# word tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\n",
    "\tfor index in range(len(tokens)):\n",
    "\t\t# stem word to each word\n",
    "\t\tstem_word = stemmer.stem(tokens[index])\n",
    "\t\t# update tokens list with stem word\n",
    "\t\ttokens[index] = stem_word\n",
    "\n",
    "\t# join list with space separator as string\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# initialize porter stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "# example text for stemming technique\n",
    "ex_stem = \"Programers program with programing languages\"\n",
    "stem_result = porter_stemmer(ex_stem)\n",
    "print(f\"Result after stemming technique :- \\n{stem_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c46b05",
   "metadata": {},
   "source": [
    "In the porter_stemmer function, we tokenized the input using word_tokenize from the nltk library. And then, apply the stem function to each of the tokenized words and update the text with stemmer words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f2805",
   "metadata": {},
   "source": [
    "**Lemmatization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcbfcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef96c772",
   "metadata": {},
   "source": [
    "The aim of usage of lemmatization is similar to the stemming technique to reduce inflection words to their original or base words. But the lemmatization process is different from the above approach.\n",
    "\n",
    "Lemmatization does not only trim the suffix characters; instead, use lexical knowledge bases to get original words. The result of lemmatization is always a meaningful word, not like stemming.\n",
    "\n",
    "The disadvantages of stemming people prefer to use lemmatization to get base or root words of original words. This preprocessing technique is also optional; we have to apply it based on our problem statement.\n",
    "\n",
    "Suppose we are doing POS (parts-of-speech) tagger problems. The original words of data have more information about data. As compared to stemming, the lemmatization speed is a little bit slow.\n",
    "\n",
    "Let's see the implementation of lemmatization using nltk library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017895d",
   "metadata": {},
   "source": [
    "**Implementation of lemmatization using nltk**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9fec2",
   "metadata": {},
   "source": [
    "In the below strip, before calling the lemmatization function, we have to initialize the object for WordNetLemmatizer to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc679c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Implementation of lemmatization using nltk\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatization\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\tResult :- string after stemming\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\tInput :- String\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\tOutput :- String\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "## Implementation of lemmatization using nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization(text):\n",
    "\t\"\"\"\n",
    "\tResult :- string after stemming\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\t# word tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\n",
    "\tfor index in range(len(tokens)):\n",
    "\t\t# lemma word\n",
    "\t\tlemma_word = lemma.lemmatize(tokens[index])\n",
    "\t\ttokens[index] = lemma_word\n",
    "\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# initialize lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "# example text for lemmatization\n",
    "ex_lemma = \"\"\"\n",
    "Programers program with programing languages\n",
    "\"\"\"\n",
    "lemma_result = lemmatization(ex_lemma)\n",
    "print(f\"Result of lemmatization \\n{lemma_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56aecc2",
   "metadata": {},
   "source": [
    "We can see the differences between the outputs of stemming and lemmatization. Programmers program programming all are different, and for languages, lemma gives meaningful words but stemming words for that are meaningless.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267808a",
   "metadata": {},
   "source": [
    "**Differences between Stemming and Lemmatization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd01231",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "Statistical method and text normalization technique.\n",
    "In the process of stemming remove the suffix of words to get a base word.\n",
    "Stemming does not always provide meaning or dictionary words  as its result.\n",
    "The speed of the stemming process is fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf0586",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "Lemmatization is also the same as stemming statistical methods and normalization techniques.\n",
    "Lemmatization follows lexical knowledge to get the root word for original one.\n",
    "The resulting words of lemmatization are always meaningful and dictionary words.\n",
    "As compared to stemming the process, the speed of lemmatization is slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af77af3",
   "metadata": {},
   "source": [
    "**Removal of Emojis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2e919",
   "metadata": {},
   "source": [
    "In today's online communication, emojis play a very crucial role.\n",
    "\n",
    "Emojis are small images. Users use these emojis to express their present feelings. We can communicate these with anyone globally. For some problem statements, we need to remove emojis from the text.\n",
    "\n",
    "Let's see on that type of problem statement how we can remove emojis.\n",
    "\n",
    "Implementation of emoji removing\n",
    "For this we take code snippets from this GitHub Repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a8e9d",
   "metadata": {},
   "source": [
    "**Implementation of emoji removing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b10ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a3d428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result text after removing emojis :- \n",
      "\n",
      "This is a test  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of emoji removing\n",
    "\n",
    "def remove_emojis(text):\n",
    "\t\"\"\"\n",
    "\tResult :- string without any emojis in it\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\temoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\twithout_emoji = emoji_pattern.sub(r'',text)\n",
    "\treturn without_emoji\n",
    "\n",
    "\n",
    "# example text for emoji removing technique\n",
    "ex_emoji = \"\"\"\n",
    "This is a test 😻 👍🏿\n",
    "\"\"\"\n",
    "# calling function\n",
    "emoji_result = remove_emojis(ex_emoji)\n",
    "print(f\"Result text after removing emojis :- \\n{emoji_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb882c41",
   "metadata": {},
   "source": [
    "**Removal of Emoticons**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e80e7",
   "metadata": {},
   "source": [
    "Emojis and emoticons are both different. An emoticon portrays a human facial expression using just keyboard characters, such as letters, numbers, and punctuation marks.\n",
    "\n",
    "This is also the same as emojis; if problem statements don't require emoticons, we can remove them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5278c6",
   "metadata": {},
   "source": [
    "**Implementation of removing of emoticons**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c766e1",
   "metadata": {},
   "source": [
    "To remove emotions from the text, we need a list of emoticons; in this GitHub Repo, we can find all emoticons as a dictionary.\n",
    "\n",
    "We take an EMOTICONS dictionary from that GitHub repo and save it in our system as emoticons_list.py. After that, import that file into our preprocessing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ae84e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73b0b22e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoticons_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of removing of emoticons\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01memoticons_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EMOTICONS\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_emoticons\u001b[39m(text):\n\u001b[0;32m      5\u001b[0m \t\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\tReturn :- string after removing emoticons\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\tInput :- string\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\tOutput :- string\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'emoticons_list'"
     ]
    }
   ],
   "source": [
    "# Implementation of removing of emoticons\n",
    "\n",
    "from emoticons_list import EMOTICONS\n",
    "def remove_emoticons(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- string after removing emoticons\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\temoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "\n",
    "\twithout_emoticons = emoticon_pattern.sub(r'',text)\n",
    "\treturn without_emoticons\n",
    "\n",
    "# example sentence for removing emoticons\n",
    "ex_emoticons = \"\"\"\n",
    "Hello this is a sentence with these 2 emoticons :-) & :-)\n",
    "\"\"\"\n",
    "emoticons_result = remove_emoticons(ex_emoticons)\n",
    "print(f\"After removing emoticons :- \\n{emoticons_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3d59b",
   "metadata": {},
   "source": [
    "**Converting Emojis to words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00050134",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0bdafd9",
   "metadata": {},
   "source": [
    "The previous section, we removed emojis from the text, but some problem statements get information from emojis.\n",
    "\n",
    "In that case, we shouldn't remove emojis.\n",
    "\n",
    "For example, if we are working on sentiment analysis on restaurant reviews data. One review is\n",
    "\n",
    "\"i ordered fried rice that is, 😋 😋\"\n",
    "another review is\n",
    "\n",
    "\"i ordered fried rice that is 😞😠\".\n",
    "If we remove emojis from these two sentences. We cannot get the user's sentiment. So, in this case, we can convert emojis into words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e30e8a",
   "metadata": {},
   "source": [
    "**Implementation of converting emoji to words using python**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1c76b",
   "metadata": {},
   "source": [
    "From this GitHub Repo, we can also get emojis words and Unicode of corresponding emojis in a dictionary.\n",
    " \n",
    "Take an EMO_UNICODE dictionary from that git and save it in a python file, then we can import the EMO_UNICODE dictionary to our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86ffd2",
   "metadata": {},
   "source": [
    "EMO_UNICODE has emoji words as a key and unicode for that value. But for converting emojis to words, we need that dictionary in reverse like unicode as key and emoji word as value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3748dddb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "304ef6d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoticons_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of converting emoji to words using python\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01memoticons_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EMO_UNICODE\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memoji_words\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m emot \u001b[38;5;129;01min\u001b[39;00m UNICODE_EMO:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'emoticons_list'"
     ]
    }
   ],
   "source": [
    "# Implementation of converting emoji to words using python\n",
    "\n",
    "from emoticons_list import EMO_UNICODE\n",
    "\n",
    "def emoji_words(text):\n",
    "\tfor emot in UNICODE_EMO:\n",
    "\t\temoji_pattern = r'('+emot+')'\n",
    "\t\t# replace\n",
    "\t\temoji_words = UNICODE_EMO[emot]\n",
    "\t\treplace_text = emoji_words.replace(\",\",\"\")\n",
    "\t\treplace_text = replace_text.replace(\":\",\"\")\n",
    "\t\treplace_text_list = replace_text.split()\n",
    "\t\temoji_name = '_'.join(replace_text_list)\n",
    "\t\ttext = re.sub(emoji_pattern, emoji_name, text)\n",
    "\treturn text\n",
    "\n",
    "\n",
    "# convert emo_unicode to unicode_emo\n",
    "UNICODE_EMO = {v: k for k, v in EMO_UNICODE.items()}\n",
    "# example text for converting emojis to words\n",
    "ex_emoji = \"\"\"\n",
    "This is a test 😻 👍🏿\n",
    "\"\"\"\n",
    "\n",
    "emoji_result = emoji_words(ex_emoji)\n",
    "print(f\"Result after converting emojis to corresponding words :- \\n{emoji_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b762ceb",
   "metadata": {},
   "source": [
    "**Converting Emoticons to words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abfcf3",
   "metadata": {},
   "source": [
    "e purpose of converting emoticons to words is also the same as converting emojis to words techniques. The only difference is here, converting emoticons to words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12a621",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88d514f5",
   "metadata": {},
   "source": [
    "**Implementation of converting emoticons to words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18e1a4",
   "metadata": {},
   "source": [
    "Take the EMOTICONS dictionary from this GitHub Repo.  We saved that dictionary of emoticons in an emoticons_list python file.\n",
    "In the below code, we import the EMOTICONS dictionary from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49199584",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoticons_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of converting emoticons to words\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01memoticons_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EMOTICONS\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memoticons_words\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m emot \u001b[38;5;129;01min\u001b[39;00m EMOTICONS:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'emoticons_list'"
     ]
    }
   ],
   "source": [
    "# Implementation of converting emoticons to words\n",
    "\n",
    "from emoticons_list import EMOTICONS\n",
    "\n",
    "def emoticons_words(text):\n",
    "\tfor emot in EMOTICONS:\n",
    "\t\temoticon_pattern = r'('+emot+')'\n",
    "\t\t# replace\n",
    "\t\temoticon_words = EMOTICONS[emot]\n",
    "\t\treplace_text = emoticon_words.replace(\",\",\"\")\n",
    "\t\treplace_text = replace_text.replace(\":\",\"\")\n",
    "\t\treplace_text_list = replace_text.split()\n",
    "\t\temoticon_name = '_'.join(replace_text_list)\n",
    "\t\ttext = re.sub(emoticon_pattern, emoticon_name, text)\n",
    "\treturn text\n",
    "\n",
    "\n",
    "# example sentence for converting  emoticons to words\n",
    "ex_emoticons = \"\"\"\n",
    "Hello this is a sentence with these 2 emoticons :-) & :-)\n",
    "\"\"\"\n",
    "emoticons_result = emoticons_words(ex_emoticons)\n",
    "print(f\"After converting emoticons to words :- \\n{emoticons_result}\")\n",
    "\n",
    "## Output:: Hello this is a sentence with these 2 emoticons Happy_face_smiley & Happy_face_smiley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fc0b0",
   "metadata": {},
   "source": [
    "**Removing of Punctuations or Special Characters**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9e83e",
   "metadata": {},
   "source": [
    "Punctuations or special characters are all characters except digits and alphabets. List of all available special characters are [!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~].  \n",
    "\n",
    "This is better to remove or convert emoticons before removing punctuations or special characters.\n",
    "\n",
    "If we apply this technique process before emoticons related techniques, we may lose emoticons from the text. So if we apply the emoticons technique, apply before removing the punctuation technique.\n",
    "\n",
    "For example, if we remove the period using the punctuation removing technique from text like \"money 20.98\", we will lose the period (.) between 20 & 98. That completely lost their meaning.\n",
    "\n",
    "So we have to focus more on choosing punctuations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745c2e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b323cf77",
   "metadata": {},
   "source": [
    "**Implementation of removing punctuations using string library**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "647cfcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result after removing punctuations :- \n",
      "\n",
      "this is an example text for punctuations like \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of removing punctuations using string library\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing punctuations\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\treturn text.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "\n",
    "# example text for removing punctuations\n",
    "ex_punct = \"\"\"\n",
    "this is an example text for punctuations like .?/*\n",
    "\"\"\"\n",
    "punct_result = remove_punctuation(ex_punct)\n",
    "print(f\"Result after removing punctuations :- \\n{punct_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8ebe1",
   "metadata": {},
   "source": [
    "**Removing of Stopwords**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd2a96",
   "metadata": {},
   "source": [
    "Stopwords are common words and irrelevant words from which we can't get any useful information for our model or problem statement.\n",
    "\n",
    "Few stopwords are \"a\", \"an\", \"the\", etc.  \n",
    "\n",
    "For example, we can ignore stop words when we work with sentiment analysis, text classification problems. But in the case of POS (Parts-Of-Speech) tagging or language translation, we have to consider whether stop words also give more information and useful words for our problem statement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d751ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b6e494",
   "metadata": {},
   "source": [
    "We can import lists of stop words from different NLP related libraries such as nltk, spacy, gensim, etc.\n",
    "\n",
    "let's see how to remove stopwords from the text by using stop words from all these three libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc451f2",
   "metadata": {},
   "source": [
    "**Implementation of removing stopwords using all stop words from nltk, spacy, gensim**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2365123a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Implementation of removing stopwords using all stop words from nltk, spacy, gensim\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Implementation of removing stopwords using all stop words from nltk, spacy, gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing stopwords\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\ttext_without_sw = []\n",
    "\t# tokenization\n",
    "\ttext_tokens = word_tokenize(text)\n",
    "\tfor word in text_tokens:\n",
    "\t\t# checking word is stopword or not\n",
    "\t\tif word not in all_stopwords:\n",
    "\t\t\ttext_without_sw.append(word)\n",
    "\n",
    "\t# joining all tokens after removing stop words\n",
    "\twithout_sw = ' '.join(text_without_sw)\n",
    "\treturn without_sw\n",
    "\n",
    "\n",
    "# list of stopwords from nltk\n",
    "stopwords_nltk = list(stopwords.words('english'))\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "# list of stopwords from spacy\n",
    "stopwords_spacy = list(sp.Defaults.stop_words)\n",
    "# list of stopwords from gensim\n",
    "stopwords_gensim = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "\n",
    "# unique stopwords from all stopwords\n",
    "all_stopwords = []\n",
    "all_stopwords.extend(stopwords_nltk)\n",
    "all_stopwords.extend(stopwords_spacy)\n",
    "all_stopwords.extend(stopwords_gensim)\n",
    "# all unique stop words\n",
    "all_stopwords = list(set(all_stopwords))\n",
    "print(f\"Total number of Stopwords :- {len(all_stopwords)}\")\n",
    "\n",
    "# example text for stop words removing\n",
    "ex_sw = \"\"\"\n",
    "this is an example text for stopwords such as a, an, the etc.\n",
    "\"\"\"\n",
    "sw_result = remove_stopwords(ex_sw)\n",
    "\n",
    "print(f\"Result after removing stopwords :- \\n{sw_result}\")\n",
    "\n",
    "## Output:: example text stopwords , , ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69036f6",
   "metadata": {},
   "source": [
    "The code mentioned above, we take stopwords from different libraries such as nltk, spacy, and gensim. \n",
    "\n",
    "And then take unique stop words from all three stop word lists. In the remove_stopwords, we check whether the tokenized word is in stop words or not; if not in stop words list, then append to the text without the stopwords list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b26bb",
   "metadata": {},
   "source": [
    "**Removing of Frequent words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52439fce",
   "metadata": {},
   "source": [
    "The above section, we removed stopwords.\n",
    "\n",
    "Stopwords are common words all over the language. These frequent words are common words of a particular domain.\n",
    "\n",
    "If we are working on any problem statement for a specific field, we can ignore common words in that domain because those frequent words don't give too much information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e275e",
   "metadata": {},
   "source": [
    "**Implementation of frequent words removing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c2674",
   "metadata": {},
   "source": [
    "Here we use the \"Counter\" function from the collection library to remove our corpus's frequent words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0090b0df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m ex_fw \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124mMachine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124mFor example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the same algorithm but it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms fed different training data so it comes up with different classification logic.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124mThis kind of like having the answer key to a math test with all the arithmetic symbols erased:\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# calling count_fw to calculate frequent words\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m FrequentWords \u001b[38;5;241m=\u001b[39m \u001b[43mfreq_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex_fw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 Frequent Words from our example text :- \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mFrequentWords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# calling remove_fw to remove frequent words from example text\u001b[39;00m\n",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36mfreq_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mReturn :- Most frequent words\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mInput :- string\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mOutput :-\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# tokenization\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(text)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m     14\u001b[0m \tcounter[word]\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "## Implementation of frequent words removing\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def freq_words(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- Most frequent words\n",
    "\tInput :- string\n",
    "\tOutput :-\n",
    "\t\"\"\"\n",
    "\t# tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfor word in tokens:\n",
    "\t\tcounter[word]= +1\n",
    "\n",
    "\tFrequentWords = []\n",
    "\t# take top 10 frequent words\n",
    "\tfor (word, word_count) in counter.most_common(10):\n",
    "\t\tFrequentWords.append(word)\n",
    "\n",
    "\treturn FrequentWords\n",
    "\n",
    "def remove_fw(text, FrequentWords):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing frequent words\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\n",
    "\ttokens = word_tokenize(text)\n",
    "\twithout_fw = []\n",
    "\tfor word in tokens:\n",
    "\t\tif word not in FrequentWords:\n",
    "\t\t\twithout_fw.append(word)\n",
    "\n",
    "\twithout_fw = ' '.join(without_fw)\n",
    "\treturn without_fw\n",
    "\n",
    "\n",
    "# initiate object for counter\n",
    "counter = Counter()\n",
    "# some random text on machine learning\n",
    "ex_fw = \"\"\"\n",
    "Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.\n",
    "For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It's the same algorithm but it's fed different training data so it comes up with different classification logic.\n",
    "Two kinds of Machine Learning Algorithms\n",
    "You can think of machine learning algorithms as falling into one of two main categories -- supervised learning and unsupervised learning. The difference is simple, but really important.\n",
    "Supervised Learning\n",
    "Let's say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there's a problem -- you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don't have your experience so they don't know how to price their houses.\n",
    "To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it's size, neighborhood, etc, and what similar houses have sold for.\n",
    "So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details -- number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:\n",
    "This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.\n",
    "To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.\n",
    "This kind of like having the answer key to a math test with all the arithmetic symbols erased:\n",
    "\"\"\"\n",
    "\n",
    "# calling count_fw to calculate frequent words\n",
    "FrequentWords = freq_words(ex_fw)\n",
    "print(f\"Top 10 Frequent Words from our example text :- \\n{FrequentWords}\")\n",
    "\n",
    "\n",
    "# calling remove_fw to remove frequent words from example text\n",
    "fw_result = remove_fw(ex_fw, FrequentWords)\n",
    "\n",
    "print(f\"Result after removing frequent words :-\\n{fw_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cc0bb",
   "metadata": {},
   "source": [
    "The above script, we defined two functions one is for counting frequent words another is to remove them from our corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb915b",
   "metadata": {},
   "source": [
    "**Removing of Rare words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54c7d6",
   "metadata": {},
   "source": [
    "Removing rare words text preprocessing technique is similar to eliminating frequent words. We can remove more irregular words from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe4934",
   "metadata": {},
   "source": [
    "**Implementation of frequent words removing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86cefb9",
   "metadata": {},
   "source": [
    "In the below script, the same as the above one, we defined two functions: finding rare words and removing them. We take only ten rare words for this sample text; this number may increase based on our text corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b1b0534",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 65) (2364956323.py, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [33]\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 65)\n"
     ]
    }
   ],
   "source": [
    "# Implementation of rare words removing\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def rare_words(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- Most Rare words\n",
    "\tInput :- string\n",
    "\tOutput :- list of rare words\n",
    "\t\"\"\"\n",
    "\t# tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfor word in tokens:\n",
    "\t\tcounter[word]= +1\n",
    "\n",
    "\tRareWords = []\n",
    "\tnumber_rare_words = 10\n",
    "\t# take top 10 frequent words\n",
    "\tfrequentWords = counter.most_common()\n",
    "\tfor (word, word_count) in frequentWords[:-number_rare_words:-1]:\n",
    "\t\tRareWords.append(word)\n",
    "\n",
    "\treturn RareWords\n",
    "\n",
    "def remove_rw(text, RareWords):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing frequent words\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\n",
    "\ttokens = word_tokenize(text)\n",
    "\twithout_rw = []\n",
    "\tfor word in tokens:\n",
    "\t\tif word not in RareWords:\n",
    "\t\t\twithout_rw.append(word)\n",
    "\n",
    "\twithout_rw = ' '.join(without_fw)\n",
    "\treturn without_rw\n",
    "# initiate object for counter\n",
    "counter = Counter()\n",
    "# some random text on machine learning\n",
    "ex_fw = \"\"\"\n",
    "Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.\n",
    "For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It's the same algorithm but it's fed different training data so it comes up with different classification logic.\n",
    "Two kinds of Machine Learning Algorithms\n",
    "You can think of machine learning algorithms as falling into one of two main categories -- supervised learning and unsupervised learning. The difference is simple, but really important.\n",
    "Supervised Learning\n",
    "Let's say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there's a problem -- you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don't have your experience so they don't know how to price their houses.\n",
    "To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it's size, neighborhood, etc, and what similar houses have sold for.\n",
    "So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details -- number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:\n",
    "This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.\n",
    "To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.\n",
    "This kind of like having the answer key to a math test with all the arithmetic symbols erased:\n",
    "\"\"\"\n",
    "# calling rare_words to calculate rare words\n",
    "RareWords = rare_words(ex_fw)\n",
    "print(f\"Top 10 Rarer Words from our example text :- \\n{RareWords}\\n\")\n",
    "\n",
    "# calling remove_fw to remove rare words from example text\n",
    "rw_result = remove_fw(ex_fw, RareWords)\n",
    "\n",
    "print(f\"Result after removing rare words :-\\n{rw_result}\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ff12e",
   "metadata": {},
   "source": [
    "**Removing single characters**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625fd65",
   "metadata": {},
   "source": [
    "After performing all text preprocessing techniques except extra spaces, removing this is better to remove a single character if there is any present in our corpus. We can remove using regex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf455c",
   "metadata": {},
   "source": [
    "**Implementation of removing single characters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb57f813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result :-\n",
      "\n",
      "this is an example of single characters like , , and .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Remove single characters\n",
    "\n",
    "def remove_single_char(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- string after removing single characters\n",
    "\tInput :- string\n",
    "\tOutput:- string\n",
    "\t\"\"\"\n",
    "\tsingle_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "\twithout_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "\treturn without_sc\n",
    "\n",
    "# example text for removing single characters\n",
    "ex_sc = \"\"\"\n",
    "this is an example of single characters like a , b , and c .\n",
    "\"\"\"\n",
    "# calling remove_sc function to remove single characters\n",
    "sc_result = remove_single_char(ex_sc)\n",
    "print(f\"Result :-\\n{sc_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fe8bf",
   "metadata": {},
   "source": [
    "**Removing Extra Whitespaces**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67574d3",
   "metadata": {},
   "source": [
    "This is the last preprocessing technique. We can not get any information from extra spaces, so that we can ignore all additional spaces such as 0ne or more newlines, tabs, extra spaces.\n",
    "\n",
    "Our suggestion is to apply this preprocessing technique at last after performing all text preprocessing techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca7452",
   "metadata": {},
   "source": [
    "**Implementation  of removing extra whitespaces**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "755d1634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result :- \n",
      " this is an extra spaces . \n"
     ]
    }
   ],
   "source": [
    "# Removing Extra Whitespaces\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- string after removing extra whitespaces\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\tspace_pattern = r'\\s+'\n",
    "\twithout_space = re.sub(pattern=space_pattern, repl=\" \", string=text)\n",
    "\treturn without_space\n",
    "\n",
    "\n",
    "# example text for removing extra spaces\n",
    "ex_space = \"\"\"\n",
    "this      is an\n",
    "extra spaces        .\n",
    "\"\"\"\n",
    "\n",
    "space_result = remove_extra_spaces(ex_space)\n",
    "print(f\"Result :- \\n{space_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a601165",
   "metadata": {},
   "source": [
    "**Process of applying all text preprocessing techniques with an Example**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a809a72",
   "metadata": {},
   "source": [
    "For this process, we are providing a complete python code in our dataaspirant github repo. You have to download this preprocessing.py file After extracting the downloaded file.\n",
    "\n",
    "Import it into our text preprocessing class from the preprocessing file. Now we will discuss how to use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc6421",
   "metadata": {},
   "source": [
    "**Implementation of Complete preprocessing techniques** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f85b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782b4886",
   "metadata": {},
   "source": [
    " The below, we apply only a few text preprocessing techniques to know how we can use the importing class.\n",
    "\n",
    "Here we are taking the Sms_spam_or_not dataset.\n",
    "\n",
    "From the dataset, we are taking a text column and converting it into a list. We initiated an object for the prepress class, which one imported from a preprocessing file.\n",
    "\n",
    "If we want to apply preprocessing techniques, send a list of sentences and a list of techniques to the preprocessing function by using the object of preprocessing.\n",
    "\n",
    "We listed out all techniques with short forms in the comment section. Please send a list of short forms of corresponding techniques as a technique list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e121994d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m===============================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mObjective: Text preprocessing techniques workflow in python\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m===============================================\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Preprocess\n\u001b[0;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_table(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMSSpamCollection\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# take text column\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "===============================================\n",
    "Objective: Text preprocessing techniques workflow in python\n",
    "Author: Sharmila.Polamuri\n",
    "Blog: https://dataaspirant.com\n",
    "Date: 2020-09-14\n",
    "===============================================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from preprocessing import Preprocess\n",
    "\n",
    "dataset = pd.read_table('SMSSpamCollection', header=None, encoding=\"utf-8\")\n",
    "# take text column\n",
    "text_column = dataset[1]\n",
    "\n",
    "sentences = text_column.tolist()\n",
    "# below technique list\n",
    "\"\"\"\n",
    "lcc = Lower case conversion\n",
    "rurls = Removing URLs\n",
    "ntw = convert numbers to words\n",
    "res = Removing Extra Spaces\n",
    "\"\"\"\n",
    "techniques = [\"lcc\", \"rurls\",\"sc\", \"ntw\", \"res\"]\n",
    "\n",
    "# remaining techniques\n",
    "\"\"\"\n",
    "\tlcc = lower case conversion\n",
    "\trht = Removing HTML tags\n",
    "\trurls = Removing Urls\n",
    "\trn = Removing Numbers\n",
    "\tntw = convert numbers to words\n",
    "\tsc = Spelling Correction\n",
    "\tata = convert accented to ASCII code\n",
    "\tsto = short_to_original\n",
    "\tec = Expanding Contractions\n",
    "\tps = Stemming (Porter Stemming)\n",
    "\tl = Lemmatization\n",
    "\tre = Removing Emojis\n",
    "\tret = Removing Emoticons\n",
    "\tew = Convert Emojis to words\n",
    "\tetw = Convert Emoticons to words\n",
    "\trp = Removing Punctuations\n",
    "\trs = Removing Stopwords\n",
    "\trfw = Removing Frequent Words\n",
    "\trrw = Removing Rare Words\n",
    "\trsc = Removing Single characters\n",
    "\tres = Removing Extra Spaces\n",
    "\"\"\"\n",
    "print(f\"******** Before preprocessing technique ******* \")\n",
    "for sent in sentences[:5]:\n",
    "\tprint(sent)\n",
    "# initiate Preprocess object\n",
    "preprocessing = Preprocess()\n",
    "\n",
    "preprocessed_text = preprocessing.preprocessing(sentences, techniques)\n",
    "print(f\"******** After preprocessing ****************\")\n",
    "for sent in preprocessed_text[:5]:\n",
    "\tprint(sent)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Output::\n",
    "******** Before preprocessing technique *******\n",
    "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "Ok lar... Joking wif u oni...\n",
    "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
    "U dun say so early hor... U c already then say...\n",
    "Nah I don't think he goes to usf, he lives around here though\n",
    "1 text Processing |################################| 5572/5572\n",
    "2 text Processing |################################| 5572/5572\n",
    "3 text Processing |################################| 5572/5572\n",
    "4 text Processing |################################| 5572/5572\n",
    "Technique Processing |################################| 4/4\n",
    "******** After preprocessing ****************\n",
    "go until jurong point , crazy.. available only in bugis n great world la e buffet ... cine there got amore wat ...\n",
    "ok lar ... joking wif u oni ...\n",
    "free entry in two a wkly comp to win fa cup final tkts 21st may 2005. text fa to eighty-seven thousand, one hundred and twenty-one to receive entry question ( std txt rate ) t & c 's apply 08452810075over18 's\n",
    "u dun say so early hor ... u c already then say ...\n",
    "nah i do n't think he goes to usf , he lives around here though\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33421a3",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f1dad",
   "metadata": {},
   "source": [
    "This article, most of the text preprocessing techniques are explained. We do not need to perform all preprocessing techniques. Just download the file and import the file in our code.\n",
    "\n",
    "All function with a list of sentences and a list of text preprocessing techniques. Focus when we select techniques and also order because the preprocessing process depends on this order of processing techniques.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
